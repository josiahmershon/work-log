## Work Log Entry: 2025-11-20

Clock in: 8:03 a.m.
Clock out: 12:01 p.m.
Clock in: 5:16 p.m.
Clock out: 7:13 p.m.

### Summary

I spent the bulk of the shift chasing down a severe memory spike reported by ops (142 GiB of 256 GiB in use, risk of OOM on our inference stack). The investigation ran ~6 hours end to end:
Pulled system telemetry (free, ps, meminfo) and triaged high-RSS processes (vLLM, uvicorn, ingest worker) to rule out user-space leaks.
Escalated to kernel memory accounting with slabtop//proc/slabinfo; discovered ~10 GiB in unreclaimable slabs dominated by ZFS caches (arc_buf_hdr, dnode_t, zfs_znode_cache, etc.).
Confirmed root cause by inspecting /proc/spl/kstat/zfs/arcstats: ARC size was ~134 GiB, explaining the disparity.
Evaluated impact/risk: normal ZFS ARC growth on long-uptime hosts, but could starve other workloads during bursts. Drafted mitigation plan (temporary shrink via /sys/module/zfs/parameters/zfs_arc_max, persistent limit through /etc/modprobe.d/zfs.conf).

Outcome: no user-space leak, averted an unnecessary service restart, and we now have a playbook entry for future memory alerts on the GPU inference node.